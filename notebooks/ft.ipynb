{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Fine-tuning for AllyIn Compass\n",
    "## Enterprise-specific query understanding and response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI client initialized\n"
     ]
    }
   ],
   "source": [
    "client = openai.OpenAI()\n",
    "print(\"âœ… OpenAI client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_positive_feedback(min_examples=8):\n",
    "    \"\"\"Load positive feedback from your app\"\"\"\n",
    "    feedback_file = Path(\"../feedback/feedback.jsonl\")\n",
    "    \n",
    "    if not feedback_file.exists():\n",
    "        print(\"âŒ No feedback file found!\")\n",
    "        return []\n",
    "    \n",
    "    positive_examples = []\n",
    "    with open(feedback_file, 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if data.get('rating', 0) > 0:  # Positive feedback only\n",
    "                    positive_examples.append({\n",
    "                        'query': data['query'],\n",
    "                        'answer': data['answer']\n",
    "                    })\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    print(f\"âœ… Found {len(positive_examples)} positive examples\")\n",
    "    \n",
    "    if len(positive_examples) < min_examples:\n",
    "        print(f\"âš ï¸ Need at least {min_examples} examples\")\n",
    "        return []\n",
    "    \n",
    "    return positive_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 10 positive examples\n"
     ]
    }
   ],
   "source": [
    "def load_positive_feedback(min_examples=5):\n",
    "    \"\"\"Load positive feedback examples for fine-tuning\"\"\"\n",
    "    feedback_file = Path(\"../feedback/feedback.jsonl\")\n",
    "    \n",
    "    if not feedback_file.exists():\n",
    "        print(\"âŒ No feedback file found!\")\n",
    "        return []\n",
    "    \n",
    "    positive_examples = []\n",
    "    with open(feedback_file, 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if data.get('rating', 0) > 0:\n",
    "                    positive_examples.append({\n",
    "                        'query': data['query'],\n",
    "                        'answer': data['answer']\n",
    "                    })\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    print(f\"âœ… Found {len(positive_examples)} positive examples\")\n",
    "    \n",
    "    if len(positive_examples) < min_examples:\n",
    "        print(f\"âš ï¸ Need at least {min_examples} examples for fine-tuning\")\n",
    "        print(f\"â³ Collect {min_examples - len(positive_examples)} more positive feedback\")\n",
    "        return []\n",
    "    \n",
    "    return positive_examples\n",
    "\n",
    "examples = load_positive_feedback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Formatted 10 training examples\n"
     ]
    }
   ],
   "source": [
    "def format_for_openai(examples):\n",
    "    \"\"\"Format for OpenAI fine-tuning\"\"\"\n",
    "    training_data = []\n",
    "    \n",
    "    system_prompt = (\n",
    "        \"You are AllyIn Compass, an enterprise AI assistant. \"\n",
    "        \"Provide precise, factual answers with specific names and numbers. \"\n",
    "        \"Use bullet points for lists. Keep responses concise and data-focused.\"\n",
    "    )\n",
    "    \n",
    "    for ex in examples:\n",
    "        training_data.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": ex['query']},\n",
    "                {\"role\": \"assistant\", \"content\": ex['answer']}\n",
    "            ]\n",
    "        })\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "if examples:\n",
    "    training_data = format_for_openai(examples)\n",
    "    print(f\"âœ… Formatted {len(training_data)} training examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Created: allyin_training_20250527_041134.jsonl\n",
      "ðŸ“¤ Uploaded file: file-4eRBGTDjvT3HBAHtECRMR7\n"
     ]
    }
   ],
   "source": [
    "if examples:\n",
    "    # Save training file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    training_file = f\"allyin_training_{timestamp}.jsonl\"\n",
    "    \n",
    "    with open(training_file, 'w') as f:\n",
    "        for item in training_data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "    \n",
    "    print(f\"ðŸ“ Created: {training_file}\")\n",
    "    \n",
    "    # Upload to OpenAI\n",
    "    with open(training_file, 'rb') as f:\n",
    "        file_response = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "    \n",
    "    file_id = file_response.id\n",
    "    print(f\"ðŸ“¤ Uploaded file: {file_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Fine-tuning started!\n",
      "ðŸ”‘ Job ID: ftjob-xZGcx49eHSqMmp7qIKarfx9U\n",
      "ðŸ“Š Status: validating_files\n",
      "ðŸŒ Monitor: https://platform.openai.com/finetune/ftjob-xZGcx49eHSqMmp7qIKarfx9U\n"
     ]
    }
   ],
   "source": [
    "if examples:\n",
    "    job = client.fine_tuning.jobs.create(\n",
    "        training_file=file_id,\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        hyperparameters={\n",
    "            \"n_epochs\": 3,\n",
    "            \"batch_size\": 16,  # Increased batch size for faster training\n",
    "            \"learning_rate_multiplier\": 0.9  # Higher learning rate\n",
    "        },\n",
    "        suffix=\"allyin-compass\"\n",
    "    )\n",
    "    \n",
    "    job_id = job.id\n",
    "    print(f\"ðŸš€ Fine-tuning started!\")\n",
    "    print(f\"ðŸ”‘ Job ID: {job_id}\")\n",
    "    print(f\"ðŸ“Š Status: {job.status}\")\n",
    "    print(f\"ðŸŒ Monitor: https://platform.openai.com/finetune/{job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Status: succeeded\n",
      "ðŸŽ‰ Success! Model: ft:gpt-4o-mini-2024-07-18:nyu:allyin-compass:Bbjlxp8T\n",
      "ðŸ’¾ Saved to config/model.txt\n"
     ]
    }
   ],
   "source": [
    "def check_status(job_id):\n",
    "    \"\"\"Check fine-tuning status\"\"\"\n",
    "    job = client.fine_tuning.jobs.retrieve(job_id)\n",
    "    \n",
    "    print(f\"ðŸ“Š Status: {job.status}\")\n",
    "    \n",
    "    if job.status == \"succeeded\":\n",
    "        model_id = job.fine_tuned_model\n",
    "        print(f\"ðŸŽ‰ Success! Model: {model_id}\")\n",
    "        \n",
    "        # Save for RAG pipeline\n",
    "        config_dir = Path(\"../config\")\n",
    "        config_dir.mkdir(exist_ok=True)\n",
    "        with open(config_dir / \"model.txt\", 'w') as f:\n",
    "            f.write(model_id)\n",
    "        print(f\"ðŸ’¾ Saved to config/model.txt\")\n",
    "        return model_id\n",
    "    \n",
    "    elif job.status == \"failed\":\n",
    "        print(f\"âŒ Failed!\")\n",
    "        if hasattr(job, 'error'):\n",
    "            print(f\"Error: {job.error}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"â³ Still running...\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Check your job status (replace with your job_id)\n",
    "# if 'ftjob-xZGcx49eHSqMmp7qIKarfx9U' in locals():\n",
    "model_id = check_status('ftjob-xZGcx49eHSqMmp7qIKarfx9U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Fine-tuned model not available for testing\n",
      "You can test it later once fine-tuning completes by running:\n",
      "test_model('your-fine-tuned-model-id', test_queries)\n"
     ]
    }
   ],
   "source": [
    "def test_model(model_id, test_queries):\n",
    "    \"\"\"Test the fine-tuned model with sample queries\"\"\"\n",
    "    \n",
    "    print(f\"Testing model: {model_id}\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_id,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are AllyIn Compass, an enterprise AI assistant that searches through company databases, documents, and knowledge graphs to provide comprehensive answers.\"},\n",
    "                    {\"role\": \"user\", \"content\": query}\n",
    "                ],\n",
    "                max_tokens=400,\n",
    "                temperature=0.1  # Low temperature for consistent responses\n",
    "            )\n",
    "            \n",
    "            print(f\"Test {i}: {query}\")\n",
    "            print(f\"Response: {response.choices[0].message.content}\")\n",
    "            print(f\"Tokens used: {response.usage.total_tokens}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error testing query '{query}': {str(e)}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"Show me compliance violations\",\n",
    "    \"What are our revenue trends?\",\n",
    "    \"Which customers need attention?\",\n",
    "    \"Find environmental issues\",\n",
    "    \"Biotech safety status\"\n",
    "]\n",
    "\n",
    "# Test the fine-tuned model\n",
    "if 'fine_tuned_model' in locals():\n",
    "    test_model(fine_tuned_model, test_queries)\n",
    "else:\n",
    "    print(\"âš ï¸ Fine-tuned model not available for testing\")\n",
    "    print(\"You can test it later once fine-tuning completes by running:\")\n",
    "    print(\"test_model('your-fine-tuned-model-id', test_queries)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  AllyIn Compass Fine-Tuning Pipeline\n",
      "==================================================\n",
      "âœ… Found 8 positive examples\n",
      "ðŸš€ Starting LoRA fine-tuning simulation...\n",
      "ðŸ“Š LoRA Configuration:\n",
      "  - Rank (r): 2\n",
      "  - Alpha: 4\n",
      "  - Dropout: 0.1\n",
      "  - Target modules: attention layers\n",
      "\n",
      "âœ… LoRA adapter saved to ../models/lora_adapter\n",
      "ðŸŽ‰ Fine-tuning simulation complete!\n",
      "\n",
      "âœ… Fine-tuning pipeline complete!\n",
      "ðŸ“ Model adapter saved at: ../models/lora_adapter\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸ§  AllyIn Compass Fine-Tuning Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Load examples\n",
    "    examples = load_positive_feedback()\n",
    "    \n",
    "    if examples:\n",
    "        # Format and save\n",
    "        formatted = format_for_finetuning(examples)\n",
    "        \n",
    "        # Simulate training\n",
    "        adapter_path = simulate_lora_finetuning()\n",
    "        \n",
    "        # Evaluate\n",
    "        # evaluate_improvement()\n",
    "        \n",
    "        print(\"\\nâœ… Fine-tuning pipeline complete!\")\n",
    "        print(f\"ðŸ“ Model adapter saved at: {adapter_path}\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ Not enough positive feedback for fine-tuning\")\n",
    "        print(\"ðŸ’¡ Collect more feedback through the UI first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allyin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
